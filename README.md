# ðŸ§  Domain-Specific Fine-Tuning with Unsloth

Fine-tune a lightweight `unsloth/Llama-3.2-1B` model on military doctrine (FM 5-0). This project covers PDF parsing, training data generation using ChatGPT 4o mini, instruction-tuning via Unsloth + LoRA, and evaluationâ€”including a semantic search demo.

---

## ðŸš€ Quickstart

```bash
# 1. Set up virtual environment
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# 2. Parse and chunk FM 5-0 PDF
python parse_pdf.py

# 3. Generate training data (ChatGPT 4o mini)
python generate_training_data.py

# 4. Convert to Unsloth format & fine-tune
python convert_to_unsloth_format.py
python finetune_unsloth.py

# 5. Evaluate base vs. fine-tuned model
python evaluate_models.py

# 6. Bonus: semantic search over FM 5-0
python search_chunks.py
```

---

## ðŸ“‚ Project Files

- `parse_pdf.py` â€” Extracts and chunks text from the FM 5-0 PDF  
- `generate_training_data.py` â€” Uses OpenAI API (ChatGPT 4o mini) to generate QA-style examples  
- `train_data.jsonl` â€” QA-style examples generated via `generate_training_data.py`  
- `convert_to_unsloth_format.py` â€” Converts QA pairs into Unsloth fine-tuning format  
- `finetune_unsloth.py` â€” Fine-tunes the model using LoRA + Hugging Face Transformers  
- `evaluate_models.py` â€” Compares outputs from base and fine-tuned models  
- `search_chunks.py` â€” Bonus: semantic search over FM 5-0 using vector embeddings  
- `fm5_chunks.json` â€” Parsed and structured FM 5-0 document chunks  
- `unsloth_dataset.jsonl` â€” Final dataset formatted for LoRA fine-tuning  
- `Build And Issues Log.pdf` â€” Setup process, bugs, and troubleshooting log  
- `Math Foundations.pdf` â€” LaTeX write-up of mathematical methods used in training or evaluation

> **Note:** Fine-tuned model weights (e.g., adapter files) are *not included* in this repository to keep it lightweight. The model can be re-trained by running `finetune_unsloth.py`.

---

## ðŸ§¾ Data Artifacts

- `fm5_chunks.json` â€” Parsed FM 5-0 text, chunked for QA generation  
- `train_data.jsonl` â€” Initial QA-style training examples (ChatGPT 4o mini)  
- `unsloth_dataset.jsonl` â€” Final data formatted for LoRA fine-tuning
